{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose is to try to create simple neural network by hand\n",
    "\n",
    "## Getting MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and loading MNIST data\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[ 0.  0.  0. ...,  9.  9.  9.]\n"
     ]
    }
   ],
   "source": [
    "#gathering data for training:\n",
    "\n",
    "#importing modules:\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "print('Fetching and loading MNIST data')\n",
    "mnist = fetch_mldata('MNIST original')\n",
    "# rescale the data, use the traditional train/test split\n",
    "X, y = mnist.data / 255., mnist.target\n",
    "X_train, X_test = X[:60000], X[60000:]\n",
    "y_train, y_test = y[:60000], y[60000:]\n",
    "\n",
    "print(X_train)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of data\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQQAAAECCAYAAAAYUakXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n9QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20205016c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_arr = np.array(X_train[40000])\n",
    "new_arr.resize((28, 28))\n",
    "print(\"Example of data\")\n",
    "plt.matshow(new_arr, cmap=plt.cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(np.array(X_train).transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network algorythm from Sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.997333\n",
      "Test set score: 0.976000\n"
     ]
    }
   ],
   "source": [
    "def run_sklearn_version():\n",
    "    # mlp = MLPClassifier(hidden_layer_sizes=(100, 100), max_iter=400, alpha=1e-4,\n",
    "    #                     solver='sgd', verbose=10, tol=1e-4, random_state=1)\n",
    "    mlp = MLPClassifier(\n",
    "        hidden_layer_sizes=(100,100), #The ith element represents the number of neurons in the ith hidden layer.\n",
    "        #max_iter=10,\n",
    "        alpha=1e-4, #?\n",
    "        solver='sgd', #‘sgd’ refers to stochastic gradient descent\n",
    "        verbose=False, #whether to print progress messages to stdout.\n",
    "        tol=1e-4, #when the loss or score is not improving by at least tol for two consecutive iterations, unless learning_rate is set to ‘adaptive’, convergence is considered to be reached and training stops\n",
    "        random_state=1, #seed used by the random number generator\n",
    "        learning_rate_init=.1) #controls the step-size in updating the weights\n",
    "\n",
    "    mlp.fit(X_train, y_train)\n",
    "    print(\"Training set score: %f\" % mlp.score(X_train, y_train))\n",
    "    print(\"Test set score: %f\" % mlp.score(X_test, y_test))\n",
    "run_sklearn_version()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network based on Coursera:\n",
    "Assuming we use 2 hidden layers, 100 neurons each\n",
    "\n",
    "Not done yet. Few problems found:\n",
    "* A3 is filled with \"1\"s for some reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A1\n",
      "[[ 59.61331549  67.30352542  68.69043905 ...,  39.36205165  35.85158966\n",
      "   31.51230247]\n",
      " [ 63.38708995  73.71598809  76.06052321 ...,  44.1564731   36.66524631\n",
      "   31.29901434]\n",
      " [ 60.30539194  71.12672836  72.71477102 ...,  42.92809835  35.62454804\n",
      "   30.8801253 ]\n",
      " ..., \n",
      " [ 57.6341929   66.20797694  65.24815441 ...,  40.78845681  36.94886324\n",
      "   32.94799559]\n",
      " [ 63.54128654  71.59764508  75.08148336 ...,  43.12587876  37.20624601\n",
      "   33.08009127]\n",
      " [ 66.12415827  74.80821186  76.51130173 ...,  41.47748558  39.35659756\n",
      "   36.33857941]]\n",
      "A2\n",
      "[[ 3182.29394588  3633.2934314   3744.29927799 ...,  2143.51145379\n",
      "   1878.70637116  1650.37125397]\n",
      " [ 3362.14892341  3841.95029089  3940.09699171 ...,  2251.90858589\n",
      "   1981.17932137  1743.10525311]\n",
      " [ 3162.79002574  3612.446777    3704.56454709 ...,  2122.6143218\n",
      "   1857.57644603  1636.86471037]\n",
      " ..., \n",
      " [ 2809.17512139  3210.506501    3307.74429352 ...,  1888.90686066\n",
      "   1659.49806977  1458.49396531]\n",
      " [ 3160.77484011  3612.8494348   3713.98807229 ...,  2124.7249854\n",
      "   1856.81915267  1629.81639015]\n",
      " [ 3042.12768252  3469.86745883  3557.63559328 ...,  2040.50428072\n",
      "   1793.15549445  1579.34629737]]\n",
      "A3\n",
      "[[ 1.  1.  1. ...,  1.  1.  1.]\n",
      " [ 1.  1.  1. ...,  1.  1.  1.]\n",
      " [ 1.  1.  1. ...,  1.  1.  1.]\n",
      " ..., \n",
      " [ 1.  1.  1. ...,  1.  1.  1.]\n",
      " [ 1.  1.  1. ...,  1.  1.  1.]\n",
      " [ 1.  1.  1. ...,  1.  1.  1.]]\n",
      "dA3\n",
      "[[ inf  inf  inf ..., -inf -inf -inf]\n",
      " [ inf  inf  inf ..., -inf -inf -inf]\n",
      " [ inf  inf  inf ..., -inf -inf -inf]\n",
      " ..., \n",
      " [ inf  inf  inf ..., -inf -inf -inf]\n",
      " [ inf  inf  inf ..., -inf -inf -inf]\n",
      " [ inf  inf  inf ..., -inf -inf -inf]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:38: RuntimeWarning: divide by zero encountered in log\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:38: RuntimeWarning: invalid value encountered in multiply\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:44: RuntimeWarning: divide by zero encountered in true_divide\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:44: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'tuple' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-4d4671abcb02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mW1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mW1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdW1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mB1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mB1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mdB1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m \u001b[0mrun_coursera_version\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-76-4d4671abcb02>\u001b[0m in \u001b[0;36mrun_coursera_version\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"dA3\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdA3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mZ3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0mdA2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdW3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdB3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinear_activation_backward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdA3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mB3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mZ3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"sigmoid\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[1;31m# Give A2 for linear_activation_backward for relu function. It will give us dA1, dW2, db2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'tuple' object is not callable"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from coursera_utils import sigmoid, sigmoid_backward, relu, relu_backward, linear_activation_backward\n",
    "\n",
    "def run_coursera_version():\n",
    "    m = 60000 #number of training examples\n",
    "    Y = y_train.transpose() # correct labels for training examples\n",
    "    \n",
    "    # FORWARD PROPAGATION\n",
    "    \n",
    "    # Preparing data\n",
    "    X = np.array(X_train).transpose()\n",
    "    # Random weights\n",
    "    # Layer 1: 784 features connected to 100 neurons\n",
    "    W1 = np.random.rand(100, 784)\n",
    "    # Biases: one for each neuron\n",
    "    B1 = np.zeros((100, 1))\n",
    "    # Layer 2: 100 neruons connected to 100 neurons\n",
    "    W2 = np.random.rand(100, 100)\n",
    "    # Biases: one for each neuron\n",
    "    B2 = np.zeros((100, 1))\n",
    "    # Layer 3: output layer. 100 neurons connected to 10 outputs\n",
    "    W3 = np.random.rand(10, 100)\n",
    "    # Biases\n",
    "    B3 = np.zeros((10, 1))\n",
    "    \n",
    "    # Calculating activation values:\n",
    "    Z1 = np.dot(W1, X) + B1\n",
    "    A1, cache1 = relu(Z1)\n",
    "    assert(A1.shape == (100, m)) # m examples, 100 activation values each\n",
    "    Z2 = np.dot(W2, A1) + B2\n",
    "    A2, cache2 = relu(Z2)\n",
    "    Z3 = np.dot(W3, A2) + B3\n",
    "    A3, cache3 = sigmoid(Z3) #Y_predicted\n",
    "    \n",
    "    # Using following loss function:\n",
    "    # J = -[y(i)log(a(i))+(1-y(i))*log(1-a(i))]\n",
    "    # where (i) means: value for i-th training example\n",
    "    cost = (-1 / m) * np.sum(np.multiply(Y, np.log(A3)) + np.multiply(1 - Y, np.log(1 - A3)))\n",
    "    cost = np.squeeze(cost) # matrix to number\n",
    "    \n",
    "    # BACKPROPAGATION\n",
    "    \n",
    "    Y = Y.reshape(1, m) # ensure it's shape is (1, m)\n",
    "    dA3 = - (np.divide(Y, A3) - np.divide(1 - Y, 1 - A3)) # derivative of cost with respect to A3\n",
    "    \n",
    "    # Give A3 for linear_activation_backward for sigmoid function. It will give us dA2, dW3, db3\n",
    "    #assert(dA3.shape() == Z3.shape())\n",
    "    print(\"A1\")\n",
    "    print(A1)\n",
    "    print(\"A2\")\n",
    "    print(A2)\n",
    "    print(\"A3\")\n",
    "    print(A3)\n",
    "    print(\"dA3\")\n",
    "    print(dA3)\n",
    "    print(Z3.shape())\n",
    "    dA2, dW3, dB3 = linear_activation_backward(dA3, ((A2, W3, B3), (Z3)), activation=\"sigmoid\")\n",
    "    # Give A2 for linear_activation_backward for relu function. It will give us dA1, dW2, db2\n",
    "    dA1, dW2, dB2 = linear_activation_backward(dA3, ((A1, W2, B2), (Z2)), activation=\"relu\")\n",
    "    # Give A1 for linear_activation_backward for relu function. It will give us dX (not necessary), dW1, db1\n",
    "    dX, dW1, dB1 = linear_activation_backward(dA3, ((X, W1, B1), (Z1)), activation=\"relu\")\n",
    "    \n",
    "    # TODO: updating weights and biases\n",
    "    W3 = W3 - learning_rate * dW3\n",
    "    B3 = B3 - learning_rate *dB3\n",
    "    \n",
    "    W2 = W2 - learning_rate * dW2\n",
    "    B2 = B2 - learning_rate *dB2\n",
    "    \n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    B1 = B1 - learning_rate *dB1\n",
    "run_coursera_version();\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
